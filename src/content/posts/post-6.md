---
title: Yet another post about AI
published: 2024-07-17
description: ''
image: ''
tags: [AI,Tech]
category: 'Rant'
draft: false 
---
Just everything is AI now? The marketing for this technology has been absolutely absurd and honestly patronizing ever since it came out. Have we not learned from blockchain? How the NFT craze ended just as quickly as it rose? Or how Bitcoin is just a big ponzi scheme? But criticize the sins, not the sinners they say. The use cases are, retarded at best for now, but the technologies are really fucking awsome.

Blockchain is undeniably a clever technology, representing a revolutionary advancement in how data can be securely and transparently recorded and shared. By creating an immutable ledger that is distributed across a network of computers, blockchain ensures that records are both transparent and resistant to tampering, providing very high levels of security and trust. This decentralized approach eliminates the need for intermediaries, reducing costs and increasing efficiency in various applications. However, despite its potential, there has not yet been a problem that requires blockchain alone to solve. Many of the touted use cases, such as supply chain transparency, secure voting systems, and decentralized finance, can be achieved through existing technologies and systems, though perhaps not with the same level of security or transparency that blockchain offers.

Sadly, the technology has really been poorly applied, like the NFT market and Bitcoin highlighting significant flaws. NFTs, initially heralded as a way to revolutionize digital ownership and art, quickly devolved into a speculative bubble, with questionable projects and rampant fraud undermining the market's credibility. Bitcoin, the flagship application of blockchain, while innovative, has been plagued by extreme volatility, illicit activities, and environmental concerns due to its energy-intensive mining process. These issues shows how, despite the brilliance of the blockchain, its applications need careful consideration and regulation to avoid misuse and to truly unlock its potential. The focus should be on identifying genuine problems that technology can uniquely address, rather than forcing it into applications where it may not be the best fit.

Solutions in need of a problem
Virtual Reality (and Augmented Reality) has so many revolutionary potentials that have yet to be fully realized. Currently, it is used in various niches such as gaming, training simulations, virtual tours, and remote collaboration but to become the transformative technology that many envisioned, gonna take decades. Despite impressive advancements, VR remains limited by factors like high costs, cumbersome hardware, and a lack of compelling, universally appealing applications. While VR experiences have improved significantly, we are still waaay far from the fully immersive, seamless virtual worlds like in 'Ready Player One.' If history repeats itself, then Apple Vision 4 will be a turning point (Iphone 4, GPT 4, GTA 4,...), which will be 2040 (singularity in 2045)

Now let's talk AI for a change. NPU is a relatively new tech that is being added to a lot of new CPUs, designed for AI-based activities. This is nothing new for GPUs, as RTX cards have had this for 7 generations. But supposedly a CPU with an NPU installed should run AI tasks faster or more efficiently, or both, than an equivalent CPU can, supposedly. Reality is, nobody, YET AGAIN, has much of an understanding of the real-world practicality that the technology can possibly provide. The hardware is new, the softwares that can utilize NPU are virtually non-existent and benchmarkers don't even know how to benchmark its benefits.

But let’s take a step back, when the first generation of RTX cards came out (20s series), the same questions arose, what are RTX for, what are the values of the RTX features? What can it do to justify the added price? Well I'm using a 2070 now and let’s just say the card aged like fine wine. Features, more extensive than ever and somehow the first generation of the hardware is still benefiting, proving it to be a good investment.

So, it’s all good? Yes, it's a cool technology that might even replace GPUs, potentially revolutionizing VR. However, its current use is somewhat pitiful. 'Using less power while being as powerful' is a promising start I guess. Though, NPU is being focused more on mobile phones and if phones can handle AI tasks with limited power, just imagine the possibilities for laptops. 

So in conclusion, no new technology is ever useless, but could really do without the stupid fucking marketing.